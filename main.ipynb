{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import DirichletPartitioner\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Globals\n",
    "fds = None\n",
    "word2idx = {}\n",
    "which_dataset = \"sentiment\"\n",
    "\n",
    "# Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=20000, embed_dim=100, hidden_dim=128, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(self.embedding(x))\n",
    "        return self.fc(hidden[-1])\n",
    "\n",
    "# Padding\n",
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len), dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "# Tokenization + Vocabulary\n",
    "def get_transforms(train_data, seq_len=50, vocab_size=20000):\n",
    "    global word2idx\n",
    "\n",
    "    def clean_and_tokenize(text):\n",
    "        if isinstance(text, list):\n",
    "            text = \" \".join(text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "        tokens = text.split()\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "        return tokens\n",
    "\n",
    "    if not word2idx:\n",
    "        counter = Counter()\n",
    "        for example in train_data:\n",
    "            text = example[\"text\"]\n",
    "            counter.update(clean_and_tokenize(text))\n",
    "        most_common = counter.most_common(vocab_size - 2)\n",
    "        word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        word2idx.update({word: idx + 2 for idx, (word, _) in enumerate(most_common)})\n",
    "\n",
    "    def tokenize(example):\n",
    "        tokens = clean_and_tokenize(example[\"text\"])\n",
    "        token_ids = [word2idx.get(token, word2idx[\"<UNK>\"]) for token in tokens]\n",
    "        padded = padding_([token_ids], seq_len)[0]\n",
    "        return torch.tensor(padded, dtype=torch.long), torch.tensor(example[\"label\"], dtype=torch.long)\n",
    "\n",
    "    return tokenize\n",
    "\n",
    "\n",
    "# Load Data\n",
    "def load_data(partition_id: int, num_partitions: int, alpha_partition: float):\n",
    "    global fds, word2idx\n",
    "\n",
    "    if fds is None:\n",
    "        partitioner = DirichletPartitioner(\n",
    "            num_partitions=num_partitions, partition_by=\"label\", alpha=alpha_partition, seed=42\n",
    "        )\n",
    "        fds = FederatedDataset(\n",
    "            dataset=\"mteb/tweet_sentiment_extraction\",\n",
    "            partitioners={\"train\": partitioner}\n",
    "        )\n",
    "\n",
    "    partition = fds.load_partition(partition_id)\n",
    "    partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "    transform_fn = get_transforms(partition_train_test[\"train\"])\n",
    "    train_dataset = TorchDatasetWrapper(partition_train_test[\"train\"], transform_fn)\n",
    "    test_dataset = TorchDatasetWrapper(partition_train_test[\"test\"], transform_fn)\n",
    "\n",
    "    trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(test_dataset, batch_size=32)\n",
    "    return trainloader, testloader\n",
    "\n",
    "# Train\n",
    "def train(net, trainloader, epochs, lr, device):\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    net.train()\n",
    "    for _ in range(epochs):\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Test\n",
    "def test(net, testloader, device):\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    total_loss, correct = 0.0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            total_loss += criterion(outputs, labels).item()\n",
    "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    accuracy = correct / total_samples if total_samples > 0 else 0.0\n",
    "    return total_loss / len(testloader), accuracy\n",
    "\n",
    "\n",
    "\n",
    "class TorchDatasetWrapper(Dataset):\n",
    "    def __init__(self, hf_dataset, transform_fn):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform_fn = transform_fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform_fn(self.dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.9426, Accuracy: 54.82%\n"
     ]
    }
   ],
   "source": [
    "trainloader, testloader = load_data(partition_id=0, num_partitions=10, alpha_partition=100)\n",
    "model = Net()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, trainloader, epochs=3, lr=0.001, device=device)\n",
    "loss, acc = test(model, testloader, device=device)\n",
    "print(f\"Test Loss: {loss:.4f}, Accuracy: {acc:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
